{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "import sys\n",
    "sys.path.remove('/home/jovyan/.imgenv-vasilyev-0/lib/python3.7/site-packages')\n",
    "sys.path.append('/home/jovyan/klenitskiy/repos/seqrec-experiments/')\n",
    "sys.path.append('/home/jovyan/klenitskiy/repos/seqrec-datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "libgomp: Invalid value for environment variable OMP_NUM_THREADS\n",
      "\n",
      "libgomp: Invalid value for environment variable OMP_NUM_THREADS\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, ModelSummary, RichProgressBar\n",
    "from replay.splitters import ColdUserRandomSplitter, NewUsersSplitter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "\n",
    "from seqrec_experiments.metrics import Evaluator\n",
    "from seqrec_experiments.lightning.datasets import CausalLMDataset, CausalLMPredictionDataset, PaddingCollateFn\n",
    "from seqrec_experiments.lightning.models import GPT4Rec\n",
    "from seqrec_experiments.lightning.modules import SeqRec\n",
    "from seqrec_experiments.postprocess import preds2recs\n",
    "from seqrec_experiments.utils import extract_validation_history\n",
    "\n",
    "from preprocessing.preparation import get_last_item, remove_last_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/jovyan/klenitskiy/data/ml-20m/ratings.csv'\n",
    "\n",
    "DATA_SAVE_PATH = 'data/ml-20m'\n",
    "MODEL_SAVE_PATH = 'models/ml-20m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000263, 4)\n",
      "138493 26744\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>924</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1094785598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>919</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1094785621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>2683</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1094785650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1</td>\n",
       "      <td>1584</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1094785656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1079</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1094785665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  item_id  rating   timestamp\n",
       "20        1      924     3.5  1094785598\n",
       "19        1      919     3.5  1094785621\n",
       "86        1     2683     3.5  1094785650\n",
       "61        1     1584     3.5  1094785656\n",
       "23        1     1079     4.0  1094785665"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "df.columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = df.sort_values(['user_id', 'timestamp'])\n",
    "print(df.shape)\n",
    "print(df.user_id.nunique(), df.item_id.nunique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = LabelEncoder()\n",
    "# df['item_id'] = encoder.fit_transform(df['item_id'])\n",
    "# mapping = dict(zip(encoder.classes_.tolist(), encoder.transform(encoder.classes_).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(DATA_SAVE_PATH, 'item_mapping.json'), 'w') as file_:\n",
    "#     json.dump(mapping, file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_splitter = NewUsersSplitter(\n",
    "    test_size=0.1, drop_cold_items=True, query_column=\"user_id\")\n",
    "train, test = test_splitter.split(df)\n",
    "\n",
    "user_counts = train.user_id.value_counts()\n",
    "user_ids = user_counts[user_counts > 1].index\n",
    "train = train[train.user_id.isin(user_ids)]\n",
    "\n",
    "validation_splitter = ColdUserRandomSplitter(\n",
    "    test_size=0.4, drop_cold_items=True, query_column=\"user_id\", seed=42)\n",
    "train, validation = validation_splitter.split(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10753324, 4) 74777 17376\n",
      "(7177832, 4) 49852 16084\n",
      "(1482915, 4) 13850 13999\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, train.user_id.nunique(), train.item_id.nunique())\n",
    "print(validation.shape, validation.user_id.nunique(), validation.item_id.nunique())\n",
    "print(test.shape, test.user_id.nunique(), test.item_id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996-01-29 00:00:00 2012-02-25 23:08:52\n",
      "1995-01-09 11:46:44 2012-02-25 23:02:42\n",
      "2012-02-25 23:09:42 2015-03-31 06:03:17\n"
     ]
    }
   ],
   "source": [
    "print(pd.to_datetime(train.timestamp.min(), unit='s'), pd.to_datetime(train.timestamp.max(), unit='s'))\n",
    "print(pd.to_datetime(validation.timestamp.min(), unit='s'), pd.to_datetime(validation.timestamp.max(), unit='s'))\n",
    "print(pd.to_datetime(test.timestamp.min(), unit='s'), pd.to_datetime(test.timestamp.max(), unit='s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = remove_last_item(test)\n",
    "test_last_item = get_last_item(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv(os.path.join(DATA_SAVE_PATH, 'train_raw.csv'), index=False)\n",
    "# validation.to_csv(os.path.join(DATA_SAVE_PATH, 'validation_raw.csv'), index=False)\n",
    "# test.to_csv(os.path.join(DATA_SAVE_PATH, 'test_raw.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 128\n",
    "\n",
    "VALIDATION_SIZE = 10000\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "TEST_BATCH_SIZE = 256\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "GPT_CONFIG = {\n",
    "    'vocab_size': 2,\n",
    "    'n_positions': 128,\n",
    "    'n_embd': 256,\n",
    "    'n_layer': 2,\n",
    "    'n_head': 2,\n",
    "}\n",
    "\n",
    "TRAINER_PARAMS = {\n",
    "    'max_epochs': 100,\n",
    "    'devices': 1,\n",
    "    'enable_checkpointing': True,\n",
    "}\n",
    "LEARNING_RATE = 1e-3\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 128])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CausalLMDataset(train, max_length=MAX_LENGTH, time_col='timestamp')\n",
    "\n",
    "validation_users = validation.user_id.unique()\n",
    "if VALIDATION_SIZE and (VALIDATION_SIZE < len (validation_users)):\n",
    "    validation_users = np.random.choice(validation_users, size=VALIDATION_SIZE, replace=False)\n",
    "eval_dataset = CausalLMPredictionDataset(validation[validation.user_id.isin(validation_users)],\n",
    "                                         max_length=MAX_LENGTH, validation_mode=True,\n",
    "                                         time_col='timestamp')\n",
    "\n",
    "collate_fn = PaddingCollateFn()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=True, num_workers=NUM_WORKERS,\n",
    "    collate_fn=PaddingCollateFn())\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset, batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False, num_workers=NUM_WORKERS,\n",
    "    collate_fn=PaddingCollateFn())\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 128, 131263])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = df.item_id.max() + 1\n",
    "\n",
    "model = GPT4Rec(GPT_CONFIG, vocab_size, add_head=True, tie_weights=True)\n",
    "outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]\n",
      "\n",
      "   | Name                         | Type       | Params\n",
      "-------------------------------------------------------------\n",
      "0  | model                        | GPT4Rec    | 35.2 M\n",
      "1  | model.embed_layer            | Embedding  | 33.6 M\n",
      "2  | model.transformer_model      | GPT2Model  | 1.6 M \n",
      "3  | model.transformer_model.wte  | Embedding  | 512   \n",
      "4  | model.transformer_model.wpe  | Embedding  | 32.8 K\n",
      "5  | model.transformer_model.drop | Dropout    | 0     \n",
      "6  | model.transformer_model.h    | ModuleList | 1.6 M \n",
      "7  | model.transformer_model.h.0  | GPT2Block  | 789 K \n",
      "8  | model.transformer_model.h.1  | GPT2Block  | 789 K \n",
      "9  | model.transformer_model.ln_f | LayerNorm  | 512   \n",
      "10 | model.head                   | Linear     | 33.6 M\n",
      "-------------------------------------------------------------\n",
      "35.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "35.2 M    Total params\n",
      "140.867   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010976f6a5444f38ae3eea03e36a4775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 38/99 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">293/293</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:02:27 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">2.08it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 31.000 val_ndcg: 0.172     </span>\n",
       "                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">val_hit_rate: 0.296 val_mrr: 0.134</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Validation</span>  <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">37/40  </span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:12 • 0:00:02</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">2.98it/s</span>                                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Epoch 38/99 \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m293/293\u001b[0m \u001b[38;5;245m0:02:27 • 0:00:00\u001b[0m \u001b[38;5;249m2.08it/s\u001b[0m \u001b[37mv_num: 31.000 val_ndcg: 0.172     \u001b[0m\n",
       "                                                                                 \u001b[37mval_hit_rate: 0.296 val_mrr: 0.134\u001b[0m\n",
       "\u001b[37mValidation\u001b[0m  \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;98;6;224m╸\u001b[0m\u001b[38;5;237m━━\u001b[0m \u001b[37m37/40  \u001b[0m \u001b[38;5;245m0:00:12 • 0:00:02\u001b[0m \u001b[38;5;249m2.98it/s\u001b[0m                                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "seqrec_module = SeqRec(model, lr=LEARNING_RATE, predict_top_k=10)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_ndcg\", mode=\"max\", patience=PATIENCE, verbose=False)\n",
    "model_summary = ModelSummary(max_depth=4)\n",
    "checkpoint = ModelCheckpoint(save_top_k=1, monitor=\"val_ndcg\", mode=\"max\", save_weights_only=True)\n",
    "pbar = RichProgressBar()\n",
    "callbacks=[early_stopping, model_summary, checkpoint, pbar]\n",
    "\n",
    "trainer = pl.Trainer(callbacks=callbacks, **TRAINER_PARAMS)\n",
    "\n",
    "trainer.fit(model=seqrec_module,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = extract_validation_history(trainer.logger.experiment.log_dir)\n",
    "display(history)\n",
    "history.set_index('epoch')['val_ndcg'].plot(figsize=(10,4), title='GPT4Rec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqrec_module.load_state_dict(torch.load(checkpoint.best_model_path)['state_dict'])\n",
    "\n",
    "predict_dataset = CausalLMPredictionDataset(test_inputs, max_length=MAX_LENGTH, time_col='timestamp')\n",
    "\n",
    "predict_loader = DataLoader(\n",
    "    predict_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, collate_fn=PaddingCollateFn())\n",
    "\n",
    "preds = trainer.predict(model=seqrec_module, dataloaders=predict_loader)\n",
    "\n",
    "recs = preds2recs(preds)\n",
    "print(recs.shape)\n",
    "recs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "evaluator = Evaluator()\n",
    "metrics = evaluator.compute_metrics(test_last_item, recs, train)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(seqrec_module.model, os.path.join(MODEL_SAVE_PATH, 'gpt_64_1_1.pt'))\n",
    "# torch.save(seqrec_module.model, os.path.join(MODEL_SAVE_PATH, 'gpt_64_2_2.pt'))\n",
    "# torch.save(seqrec_module.model, os.path.join(MODEL_SAVE_PATH, 'gpt_256_2_2.pt'))\n",
    "\n",
    "# torch.save(seqrec_module.model, os.path.join(MODEL_SAVE_PATH, 'gpt_64_2_2_raw.pt'))\n",
    "# torch.save(seqrec_module.model, os.path.join(MODEL_SAVE_PATH, 'gpt_256_2_2_raw.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(seqrec_module.model.state_dict(), os.path.join(MODEL_SAVE_PATH, 'gpt_64_1_1.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "klenitskiy_seqrec",
   "language": "python",
   "name": "klenitskiy_seqrec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
